{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk  # Natural Language Toolkit\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk  # Import specific NLTK functions\n",
    "from nltk.tree import Tree  # Import the Tree data structure\n",
    "\n",
    "# Download required NLTK data files for tokenization, POS tagging, and Named Entity Recognition (NER)\n",
    "nltk.download('punkt')  # Tokenizer models\n",
    "nltk.download('averaged_perceptron_tagger')  # Part-of-Speech (POS) tagging models\n",
    "nltk.download('maxent_ne_chunker')  # Named Entity Recognition (NER) chunker\n",
    "nltk.download('words')  # Word list used by the NER chunker\n",
    "\n",
    "def remove_names_and_locations(text):\n",
    "    \"\"\"\n",
    "    Remove names of persons and locations from the given text using Named Entity Recognition (NER).\n",
    "\n",
    "    :param text: The input text to process\n",
    "    :return: Text with names and locations replaced by an empty string\n",
    "    \"\"\"\n",
    "    # Tokenize the text into individual words\n",
    "    tokenized_text = word_tokenize(text)\n",
    "    \n",
    "    # Perform Part-of-Speech (POS) tagging on the tokenized text\n",
    "    tagged_text = pos_tag(tokenized_text)\n",
    "    \n",
    "    # Perform Named Entity Recognition (NER) to identify named entities in the tagged text\n",
    "    chunked_text = ne_chunk(tagged_text)\n",
    "\n",
    "    def extract_entity_names_and_locations(t):\n",
    "        \"\"\"\n",
    "        Recursively extract entity names labeled as 'PERSON' or 'GPE' from the NER tree structure.\n",
    "\n",
    "        :param t: NER tree node\n",
    "        :return: List of names and locations recognized as 'PERSON' or 'GPE'\n",
    "        \"\"\"\n",
    "        entity_names_and_locations = []\n",
    "        \n",
    "        # Check if the node has a label (is a subtree)\n",
    "        if hasattr(t, 'label') and t.label:\n",
    "            # If the label is 'PERSON' or 'GPE', it's a named entity representing a person's name or location\n",
    "            if t.label() in ['PERSON', 'GPE']:\n",
    "                # Join all child tokens (words) to form the complete entity name\n",
    "                entity_names_and_locations.append(' '.join([child[0] for child in t]))\n",
    "            else:\n",
    "                # Recursively check each child node in the tree\n",
    "                for child in t:\n",
    "                    entity_names_and_locations.extend(extract_entity_names_and_locations(child))\n",
    "        return entity_names_and_locations\n",
    "\n",
    "    # Extract all names and locations recognized as 'PERSON' or 'GPE' from the chunked NER tree\n",
    "    entity_names_and_locations = []\n",
    "    for tree in chunked_text:\n",
    "        entity_names_and_locations.extend(extract_entity_names_and_locations(tree))\n",
    "\n",
    "    # Remove each recognized name and location from the original text by replacing it with an empty string\n",
    "    cleaned_text = text\n",
    "    for entity in entity_names_and_locations:\n",
    "        cleaned_text = cleaned_text.replace(entity, '')\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "text = \"Hi I am Jason and I live in Toronto, Iâ€™m not feeling that well today because I have a runny nose and a headache.\"\n",
    "cleaned_text = remove_names_and_locations(text)\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "\n",
    "# Define the base URL of the MedlinePlus articles\n",
    "base_url = 'https://medlineplus.gov/ency/article/'\n",
    "\n",
    "# Define the directory to save the disease definitions\n",
    "save_dir = './data/medlineplus/'\n",
    "\n",
    "# Ensure the save directory exists\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "def get_disease_info(article_id):\n",
    "    try:\n",
    "        disease_url = f'{base_url}{article_id}.htm'\n",
    "        response = requests.get(disease_url)\n",
    "        response.raise_for_status()  # Raise an HTTPError for bad responses\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract the disease name and definition\n",
    "        disease_name = soup.find('h1').text.strip()\n",
    "        print(f\"Fetching info for: {disease_name}\")  # Debugging print\n",
    "        definition_section = soup.find('div', class_='section')\n",
    "        definition = definition_section.text.strip() if definition_section else \"No definition found.\"\n",
    "        \n",
    "        return {\n",
    "            'name': disease_name,\n",
    "            'definition': definition\n",
    "        }\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching disease info from {disease_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def save_disease_info(disease_info):\n",
    "    # Create a valid filename by removing characters that are not allowed in filenames\n",
    "    filename = \"\".join(c for c in disease_info['name'] if c.isalnum() or c in (' ', '_')).rstrip()\n",
    "    filepath = os.path.join(save_dir, f\"{filename}.txt\")\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Name: {disease_info['name']}\\n\")\n",
    "        file.write(f\"Definition: {disease_info['definition']}\\n\")\n",
    "    \n",
    "    print(f\"Saved: {filepath}\")\n",
    "\n",
    "def main():\n",
    "    # Example range of article IDs to iterate through\n",
    "    for article_id in range(0, 1000):  # Adjust range as needed\n",
    "        article_id_str = f'{article_id:06}'  # Zero-pad to match the article ID format\n",
    "        disease_info = get_disease_info(article_id_str)\n",
    "        if disease_info:\n",
    "            save_disease_info(disease_info)\n",
    "        time.sleep(1)  # Add a delay to avoid overloading the server\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import voyageai\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Use Voyage AI API KEY\n",
    "api_key = \"VOYAGEAI_API_KEY\"\n",
    "\n",
    "vo = voyageai.Client(api_key=api_key)\n",
    "# This will automatically use the environment variable VOYAGE_API_KEY.\n",
    "# Alternatively, you can use vo = voyageai.Client(api_key=\"<your secret key>\")\n",
    "\n",
    "result = vo.embed([\"hello world\"], model=\"voyage-large-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a distinct number of drug categories\n",
    "distinct_drug_categories = [\n",
    "    'Analgesics',\n",
    "    'Antibiotics',\n",
    "    'Antifungal Agents',\n",
    "    'Antiviral Agents',\n",
    "    'Antipyretics',\n",
    "    'Antiseptics',\n",
    "    'Mood Stabilizers',\n",
    "    'Anti-Inflammatory Agents',\n",
    "    'Anticoagulants',\n",
    "    'Antihistamines',\n",
    "    'Diuretics',\n",
    "    'Laxatives',\n",
    "    'Bronchodilators',\n",
    "    'Anticonvulsants',\n",
    "    'Antidepressants'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_vector(list_of_text):\n",
    "    list_of_text: list\n",
    "    if type(list_of_text) == list:\n",
    "        result = vo.embed(list_of_text, model=\"voyage-large-2\")\n",
    "    else:\n",
    "        print(\"The input must be a list\")\n",
    "        return TypeError\n",
    "    return result.embeddings\n",
    "\n",
    "def best_drug_category(categories, standard_list):\n",
    "    vecs = embed_vector([categories]+standard_list)\n",
    "    vec_array = np.array(vecs)\n",
    "\n",
    "    # Calculate cosine similarity\n",
    "    similarity_matrix = cosine_similarity(vec_array)\n",
    "\n",
    "    first_line_similarities = similarity_matrix[0, 1:]\n",
    "\n",
    "    # Find the index of the highest similarity\n",
    "    best_fit_index = np.argmax(first_line_similarities)\n",
    "\n",
    "    # Get the corresponding category\n",
    "    best_fit_category = distinct_drug_categories[best_fit_index]\n",
    "\n",
    "    return best_fit_category\n",
    "\n",
    "def plot_variance_distribution(data):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.histplot(data, bins=20, kde=True)\n",
    "    plt.title('Distribution of Variance Values (without self-similarity)')\n",
    "    plt.xlabel('Variance')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Base URL for DrugBank\n",
    "base_url = \"https://go.drugbank.com/drugs/\"\n",
    "\n",
    "# Range of DrugBank IDs to scrape\n",
    "start_id = 1000\n",
    "end_id = 1500  # Example range, adjust as needed\n",
    "\n",
    "# Initialize a list to store drug data\n",
    "drugs_data = []\n",
    "\n",
    "for i in range(start_id, end_id + 1):\n",
    "    drug_id = f\"DB{i:05d}\"\n",
    "    url = f\"{base_url}{drug_id}\"\n",
    "    response = requests.get(url)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        \n",
    "        # Extract the drug ID\n",
    "        drug_id_meta = soup.find('meta', attrs={'name': 'dc.identifier'})\n",
    "        drug_id_value = drug_id_meta['content'] if drug_id_meta else 'N/A'\n",
    "        \n",
    "        # Extract the drug name\n",
    "        drug_name_meta = soup.find('meta', attrs={'name': 'dc.title'})\n",
    "        drug_name = drug_name_meta['content'] if drug_name_meta else 'N/A'\n",
    "        \n",
    "        # Extract the drug description\n",
    "        description_meta = soup.find('meta', attrs={'name': 'description'})\n",
    "        description = description_meta['content'] if description_meta else 'N/A'\n",
    "        \n",
    "        # Extract additional fields by looking for dt/dd pairs\n",
    "        def get_text_for_label(label):\n",
    "            tag = soup.find('dt', text=label)\n",
    "            if tag:\n",
    "                next_tag = tag.find_next_sibling('dd')\n",
    "                if next_tag:\n",
    "                    return next_tag.text.strip()\n",
    "            return 'N/A'\n",
    "\n",
    "        # Data to collect\n",
    "        mechanism_of_action = get_text_for_label('Mechanism of action')\n",
    "        indication = get_text_for_label('Indication')\n",
    "        pharmacodynamics = get_text_for_label('Pharmacodynamics')\n",
    "        absorption = get_text_for_label('Absorption')\n",
    "        volume_of_distribution = get_text_for_label('Volume of distribution')\n",
    "        protein_binding = get_text_for_label('Protein binding')\n",
    "        metabolism = get_text_for_label('Metabolism')\n",
    "        drug_categories = get_text_for_label('Drug Categories')\n",
    "\n",
    "        # Append the data to the list\n",
    "        drugs_data.append({\n",
    "            'DrugBank ID': drug_id_value,\n",
    "            'Name': drug_name,\n",
    "            'Description': description,\n",
    "            'Mechanism of Action': mechanism_of_action,\n",
    "            'Indication': indication,\n",
    "            'Pharmacodynamics': pharmacodynamics,\n",
    "            'Absorption': absorption,\n",
    "            'Volume of Distribution': volume_of_distribution,\n",
    "            'Protein Binding': protein_binding,\n",
    "            'Metabolism': metabolism,\n",
    "            'Drug Categories': drug_categories,\n",
    "            # Add more fields as necessary\n",
    "        })\n",
    "        print(f\"Gathered data for {drug_id}\")\n",
    "        # Just so not to overwhelm the server \n",
    "        time.sleep(0.01)\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data for {drug_id}\")\n",
    "\n",
    "# Convert the list to a pandas DataFrame\n",
    "df = pd.DataFrame(drugs_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list to a pandas DataFrame\n",
    "df = pd.DataFrame(drugs_data)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('drugbank_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the drugs into their respective categories\n",
    "df[\"one_category\"] = [best_drug_category(x, distinct_drug_categories) for x in df[\"Drug Categories\"]]\n",
    "df.one_category.value_counts()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-line test\n",
    "# Fetch data\n",
    "source = [x for x in list(df.iloc[1])[1:] if x.lower().strip() != \"not available\"]\n",
    "vectors = embed_vector(source)\n",
    "\n",
    "# Convert the list of vectors to a numpy array\n",
    "vector_array = np.array(vectors)\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "similarity_matrix = cosine_similarity(vector_array)\n",
    "\n",
    "# Print the similarity matrix\n",
    "# print(similarity_matrix)\n",
    "\n",
    "# Create a heatmap to visualize the similarity matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=.5)\n",
    "plt.title('Cosine Similarity Heatmap')\n",
    "plt.xlabel('Vector Index')\n",
    "plt.ylabel('Vector Index')\n",
    "plt.show()\n",
    "\n",
    "def analyze_similarity_matrix_without_self(similarity_matrix, plot = False):\n",
    "    # Remove self-similarity by setting the diagonal to NaN\n",
    "    np.fill_diagonal(similarity_matrix, np.nan)\n",
    "    \n",
    "    # Calculate the variance of the similarity matrix ignoring NaN values\n",
    "    variance = np.nanvar(similarity_matrix)\n",
    "    \n",
    "    # Flatten the similarity matrix to get the distribution of values, excluding NaN values\n",
    "    distribution = similarity_matrix.flatten()\n",
    "    distribution = distribution[~np.isnan(distribution)]\n",
    "    \n",
    "    if plot:    \n",
    "        # Plot the distribution as a histogram\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        sns.histplot(distribution, bins=20, kde=True)\n",
    "        plt.title('Distribution of Similarity Values (without self-similarity)')\n",
    "        plt.xlabel('Similarity Value')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.show()\n",
    "    else:\n",
    "        return variance\n",
    "    return variance\n",
    "\n",
    "vector_array = np.array(vectors)\n",
    "similarity_matrix = cosine_similarity(vector_array)\n",
    "\n",
    "variance = analyze_similarity_matrix_without_self(similarity_matrix, plot=True)\n",
    "print(f\"Variance of the similarity matrix (without self-similarity): {variance}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate and add the variance column to the DF\n",
    "df[\"sim_variance\"] = [analyze_similarity_matrix_without_self(\n",
    "    cosine_similarity(\n",
    "        np.array(\n",
    "            embed_vector(\n",
    "                [i for i in list(df.iloc[x]) if i.strip().lower() != \"not available\"]\n",
    "                )\n",
    "            )\n",
    "        )\n",
    "    ) for x in range(len(df))]\n",
    "plot_variance_distribution(df.sim_variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./drugbank_data.csv\")\n",
    "# Fit the drugs into their respective categories\n",
    "df[\"one_category\"] = [best_drug_category(x, distinct_drug_categories) for x in df[\"Drug Categories\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "label_column = \"Name\"\n",
    "\n",
    "def chunk_list(lst, chunk_size, token_limit):\n",
    "    \"\"\"Yield successive chunks from lst with a limit on chunk size and token count.\"\"\"\n",
    "    chunk = []\n",
    "    total_tokens = 0\n",
    "    for text in lst:\n",
    "        token_count = len(text.split())\n",
    "        if len(chunk) >= chunk_size or (total_tokens + token_count) > token_limit:\n",
    "            yield chunk\n",
    "            chunk = []\n",
    "            total_tokens = 0\n",
    "        chunk.append(text)\n",
    "        total_tokens += token_count\n",
    "    if chunk:\n",
    "        yield chunk\n",
    "\n",
    "def transform_text_columns_to_embeddings(data, text_columns, chunk_size=128, token_limit=6000):\n",
    "    # Concatenate all text columns into a single string for each row\n",
    "    combined_text = data[text_columns].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)\n",
    "\n",
    "    combined_text.replace(\"Not Available\", \"\")\n",
    "    \n",
    "    embeddings = []\n",
    "    \n",
    "    # Process the text data in chunks to avoid batch size and token limits\n",
    "    for text_chunk in chunk_list(combined_text.tolist(), chunk_size, token_limit):\n",
    "        chunk_embeddings = embed_vector(text_chunk)\n",
    "        embeddings.extend(chunk_embeddings)\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# Define the text columns (assuming the text columns are all except 'DrugBank ID')\n",
    "text_columns = df.columns.drop('DrugBank ID')\n",
    "\n",
    "# Transform the text columns into vector embeddings\n",
    "embeddings = transform_text_columns_to_embeddings(df, text_columns)\n",
    "\n",
    "# Extract the labels\n",
    "labels = df[label_column]\n",
    "categories = df[\"one_category\"]\n",
    "\n",
    "def perform_pca(embeddings, n_components=2):\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca_result = pca.fit_transform(embeddings)\n",
    "    return pca_result\n",
    "\n",
    "# Perform PCA to reduce the dimensionality to 2 components\n",
    "pca_result = perform_pca(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data = pd.DataFrame({\n",
    "        'Principal Component 1': pca_result[:, 0],\n",
    "        'Principal Component 2': pca_result[:, 1],\n",
    "        'Label': labels,\n",
    "        'Category': categories\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot PCA results with Matplotlib and mplcursors\n",
    "def plot_pca_result_with_labels(pca_result, labels):\n",
    "    df = pd.DataFrame({\n",
    "        'Principal Component 1': pca_result[:, 0],\n",
    "        'Principal Component 2': pca_result[:, 1],\n",
    "        'Label': labels\n",
    "    })\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(\n",
    "        df['Principal Component 1'], df['Principal Component 2'],\n",
    "        c=pd.factorize(df['Label'])[0], cmap='viridis', alpha=0.7\n",
    "    )\n",
    "    \n",
    "    plt.title('PCA of Text Embeddings')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_pca_result_with_labels(pca_result, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Directory to save text files\n",
    "output_dir = './data/drugbank/'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to save each row as a text file\n",
    "def save_row_as_text(row, output_dir):\n",
    "    if row[\"DrugBank ID\"] != \"N/A\":\n",
    "        file_name = f\"{row['DrugBank ID']}.txt\"\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        with open(file_path, 'w') as file:\n",
    "            for column, value in row.items():\n",
    "                file.write(f\"{column}: {value}\\n\")\n",
    "\n",
    "# Save each row in the dataframe as a text file\n",
    "df.apply(lambda row: save_row_as_text(row, output_dir), axis=1)\n",
    "\n",
    "# List the created files to verify\n",
    "os.listdir(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groq import Groq\n",
    "from openai import OpenAI\n",
    "\n",
    "def get_drug_category(drug_name, client_type=\"openai\"):\n",
    "    if client_type == \"groq\":\n",
    "        client = Groq(\n",
    "            api_key=\"<GROQ_API_KEY>\"\n",
    "        )\n",
    "        model_type = \"<MODEL_TYPE>\"\n",
    "    elif client_type == \"openai\":\n",
    "        client = OpenAI(\n",
    "            api_key=os.getenv(\"OPENAI_KEY\")\n",
    "        )\n",
    "        model_type = \"gpt-4o\"\n",
    "    else: \n",
    "        print(\"The `client_type` parameter only supports `groq` and `openai`.\")\n",
    "        raise TypeError\n",
    "    completion = client.chat.completions.create(\n",
    "        model=model_type,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a pharmacology expert, you will help me categorize the drugs into their respective category.\\\n",
    "                For example: input-'Ibuprofen', output-'Analgesics'\\\n",
    "                ONLY select from these categories:\\\n",
    "                [Analgesics, Antibiotics, Antivirals, Antifungals, Antihypertensives, Antidiabetics, Statins, Antidepressants, Antipsychotics, Bronchodilators, Unknown]\\\n",
    "                ONLY return the output category and nothing else.\"},\n",
    "            {\"role\": \"user\", \"content\": drug_name}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Get the content of the response\n",
    "    result = completion.choices[0].message.content\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data[\"category\"] = \"\"\n",
    "total = len(pca_data)\n",
    "for i in range(len(pca_data)):\n",
    "    pca_data.loc[i, \"category\"] = get_drug_category(pca_data.Label.loc[i], client_type=\"openai\")\n",
    "    print(f\"{round(100*i/total, 2)}%\", end=\"\\r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_data.to_csv(\"./drug_bank_pca_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
