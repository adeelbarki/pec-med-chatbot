{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data uploader\n",
    "import os\n",
    "import boto3\n",
    "from botocore.exceptions import NoCredentialsError\n",
    "\n",
    "def upload_to_s3(file_name, bucket, object_name=None, aws_access_key_id=None, aws_secret_access_key=None, region_name=None):\n",
    "    \"\"\"\n",
    "    Upload a file to an S3 bucket using put_object\n",
    "\n",
    "    :param file_name: File to upload\n",
    "    :param bucket: Bucket to upload to\n",
    "    :param object_name: S3 object name. If not specified, file_name is used\n",
    "    :param aws_access_key_id: AWS access key ID\n",
    "    :param aws_secret_access_key: AWS secret access key\n",
    "    :param region_name: AWS region name\n",
    "    :return: True if file was uploaded, else False\n",
    "    \"\"\"\n",
    "    \n",
    "    # If S3 object_name was not specified, use file_name\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "\n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    # Upload the file\n",
    "    with open(file_name, 'rb') as file_data:\n",
    "        s3_client.put_object(Bucket=bucket, Key=object_name, Body=file_data)\n",
    "    print(f\"File {file_name} uploaded to {bucket}/{object_name}\")\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_name = \"<FILENAME>\"  # Local file path\n",
    "bucket_name = \"<BUCKET_NAME>\"\n",
    "object_name = f\"raw/{file_name.split('/')[-1]}\" \n",
    "print(object_name)\n",
    "\n",
    "aws_access_key_id = '<AWS_ACCESS_KEY>'\n",
    "aws_secret_access_key = '<AWS_SECRET_KEY>'\n",
    "region_name = 'AWS_REGION'  # e.g., 'us-west-1'\n",
    "directory_path = '<DIR_PATH>'\n",
    "\n",
    "upload_to_s3(directory_path, bucket_name, aws_access_key_id, aws_secret_access_key, region_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import os\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def upload_to_s3(file_name, bucket, object_name=None, aws_access_key_id=None, aws_secret_access_key=None, region_name=None):\n",
    "    # Initialize S3 client\n",
    "    s3_client = boto3.client('s3',\n",
    "                             aws_access_key_id=aws_access_key_id,\n",
    "                             aws_secret_access_key=aws_secret_access_key,\n",
    "                             region_name=region_name)\n",
    "    try:\n",
    "        s3_client.upload_file(file_name, bucket, object_name)\n",
    "        print(f\"File {file_name} uploaded to {bucket}/{object_name}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to upload {file_name} to {bucket}/{object_name}: {e}\")\n",
    "        return False\n",
    "\n",
    "def upload_files_concurrently(directory, bucket, aws_access_key_id=None, aws_secret_access_key=None, region_name=None):\n",
    "    # Generate list of files from the directory\n",
    "    file_names = [os.path.join(directory, f) for f in os.listdir(directory) if os.path.isfile(os.path.join(directory, f))]\n",
    "    \n",
    "    # Create a list to hold futures\n",
    "    futures = []\n",
    "    \n",
    "    # Use ThreadPoolExecutor to upload files concurrently\n",
    "    with ThreadPoolExecutor() as executor:\n",
    "        for file_name in file_names:\n",
    "            object_name = f\"raw/{os.path.basename(file_name)}\"  # S3 object name\n",
    "            futures.append(executor.submit(upload_to_s3, file_name, bucket, object_name, aws_access_key_id, aws_secret_access_key, region_name))\n",
    "        \n",
    "        # Ensure all futures are completed\n",
    "        for future in as_completed(futures):\n",
    "            future.result()  # This will raise an exception if the function call failed\n",
    "\n",
    "# Directory containing files to upload\n",
    "directory_path = \"<FILENAME>\"\n",
    "bucket_name = \"<BUCKET_NAME>\"\n",
    "aws_access_key_id = '<AWS_ACCESS_KEY>'\n",
    "aws_secret_access_key = '<AWS_SECRET_KEY>'\n",
    "region_name = '<AWS_REGION>'  # e.g., 'us-west-1'\n",
    "\n",
    "# Upload all files concurrently\n",
    "upload_files_concurrently(directory_path, bucket_name, aws_access_key_id, aws_secret_access_key, region_name)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
