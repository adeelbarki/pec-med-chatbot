{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_voyageai import VoyageAIEmbeddings\n",
    "import os\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from pinecone import Pinecone, ServerlessSpec\n",
    "from langchain_openai import ChatOpenAI\n",
    "import openai\n",
    "from langchain.chains import LLMChain, RetrievalQA\n",
    "import time\n",
    "import re\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.schema import HumanMessage\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "os.environ[\"VOYAGE_AI_API_KEY\"] = \"<VOYAGEAI_API_KEY>\"\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"<PINECONE_API_KEY>\"\n",
    "\n",
    "model_name = \"voyage-large-2\"  \n",
    "embeddings = VoyageAIEmbeddings(\n",
    "    model=model_name,  \n",
    "    voyage_api_key=os.environ[\"VOYAGE_AI_API_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate pre-signed URL\n",
    "def generate_presigned_url(s3_uri):\n",
    "    # Parse the S3 URI\n",
    "    parsed_url = urlparse(s3_uri)\n",
    "    bucket_name = parsed_url.netloc\n",
    "    object_key = parsed_url.path.lstrip('/')\n",
    "    \n",
    "    # Create an S3 client\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Generate a pre-signed URL for the S3 object\n",
    "    presigned_url = s3_client.generate_presigned_url(\n",
    "        'get_object',\n",
    "        Params={'Bucket': bucket_name, 'Key': object_key},\n",
    "        ExpiresIn=3600  # URL expiration time in seconds\n",
    "    )\n",
    "    return presigned_url\n",
    "\n",
    "# Function to retrieve documents, generate URLs, and format the response\n",
    "def retrieve_and_format_response(query, retriever, llm):\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    \n",
    "    formatted_docs = []\n",
    "    for doc in docs:\n",
    "        content_data = doc.page_content\n",
    "        s3_uri = doc.metadata['id']\n",
    "        s3_gen_url = generate_presigned_url(s3_uri)\n",
    "        formatted_doc = f\"{content_data}\\n\\n[More Info]({s3_gen_url})\"\n",
    "        formatted_docs.append(formatted_doc)\n",
    "    \n",
    "    combined_content = \"\\n\\n\".join(formatted_docs)\n",
    "    # print(combined_content)\n",
    "    \n",
    "    # Create a prompt for the LLM to generate an explanation based on the retrieved content\n",
    "    prompt = f\"Instruction: Based on the following information, provide a summarized & concise explanation using a couple of sentences. \\\n",
    "               Only respond with the information relevant to the user query {query}, if there are none, make sure you say 'I don't know, I did not find the relevant data in the knowledge base.' \\\n",
    "               In the event that there's relevant info, make sure to attach the download button at the very end: \\n\\n[More Info]({s3_gen_url}) \\\n",
    "               Context: {combined_content}\"\n",
    "    \n",
    "    # Create the messages for the LLM input\n",
    "    messages = [HumanMessage(content=prompt)]\n",
    "    \n",
    "    # Generate the response using the LLM\n",
    "    response = llm(messages=messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PINECONE\n",
    "pc = Pinecone(api_key=os.environ.get(\"PINECONE_API_KEY\"))\n",
    "\n",
    "index_name = \"healthai-vector-embedding\"\n",
    "\n",
    "# Retriever\n",
    "docsearch = PineconeVectorStore.from_existing_index(\n",
    "    index_name=index_name,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# Initialize LLM\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", openai_api_key=\"<OPENAI_API_KEY\")\n",
    "# Initialize Memory\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query1 = \"What is Cyclacillin?\"\n",
    "query2 = \"What drugs are being used to treat HIV?\"\n",
    "docs = docsearch.as_retriever().get_relevant_documents(query2)\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "response = retrieve_and_format_response(query1, docsearch.as_retriever(), llm=llm)\n",
    "\n",
    "# Display the formatted response as Markdown\n",
    "display(Markdown(response[\"answer\"]))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
